# On The Effectiveness-Fluency Trade-Off In LLM Conditioning: A Systematic Study
In this repository we include all the auxiliary files used in our experiments and the instructions to create them. The references mentioned here can be found in the Bibliography of the paper.
## Reproducibility Statement
In order to make our work easy to reproduce, we have relied on publicly available tools and data, we provide a full list of the prompts and concepts we used, specified the hyperparameters we chose, and we will publish the code. Detailed information regarding the generic prompts, concept-inducing prompts, and toxicity mitigating prompts used in our experiments can be found in generic\_prompts.txt, concept-inducing-prompts\_templates.txt, and detox\_prompts.txt, respectively. Specifics concerning the interaction with instruction-tuned models, including chat templates and their structure at both training and inference time, are provided in chat\_templates.txt. The methodology for generating our dataset of concepts is detailed in concept-generating\_prompts.txt. The details of LLM-as-a-judge evaluation and instructions are given in concept-inducing-prompts\_templates.txt, the related human validation in directly in the main text.
The definitions and examples for our low-cost key observables (Sentence-BERT embeddings, perplexity and corpus based measures) are thoroughly explained in the Appendix of the paper.
## Resource Files
### concepts.txt
The list of concepts used in the concept injection experiments. These were taken form Fedzechkina et al. (2025). Among all the concepts present in the dataset, we extracted 10 categories, made up of 5 members each, for a total of 60 injection concepts (we used both the members and categories as concepts). We selected a diversified set of concepts, ranging from objects, to animals, to activities.
### concepts-generating_prompts.txt
The two prompts used to generate the training set of concepts-related sentences. For examples, the following are a fact and a story prompts (out of 400) for the concept _Carrot_. Fact: "Carrots are low in calories and high in fiber, making them a popular choice for those following a healthy diet." Story: "The carrot, with its crunchy texture and sweet taste, was the secret ingredient that brought an old family recipe to life, reuniting a long-lost family during a heartwarming reunion."
### generic_prompts.txt
The set of short, generic prompts fed to the models to generate both the base and steered continuations is created by prompting Qwen3-14B-IT with the command: “Write generic and descriptive prompts, made of 3 to 5 words, along the lines of 'Describe a place.', 'Tell a story.', 'Set a scene.' or 'Explain a moment.'". We collected 100 of them and sampled them randomly at inference time.
### concept-inducing-prompts_templates.txt
The 10 prompt templates that are pre-pended to the generic prompts. The X is substituted with the entries from concepts.txt and corrected for concordance. The concept is therefore explicitly present in the instruction and guides the model to act as an expert on the topic.
### detox_prompts.txt
The 11 prompts used in the prompting scenario (and prepended to the TET prompts) to mitigate the toxicity. They are taken from Suau et al. (2024)
### chat_template.txt
The instruction-tuned models are always used with the extended thinking mode disabled. The chat templates are always explicitly instantiated, at both training and inference time. The structure is identical for the two families of models employed, namely Qwen3 and Smollm3. At training time, when the models are provided a sample from the training set, the chat template has the structure "Training chat template". At inference time, when only a prompt is provided and the conditioned model is allowed to generate a continuation, we use the "Inference chat template".
### LLM-as-a-judge_prompts.txt
The instructions provided to the LLM-as-a-judge to score the continuations, inspired from  Wu et al. (2025). In their paper, the LLM is asked to first provide an explanation for its score, and then generate the score. We found a larger agreement with human annotations by asking for the score before the explanation. Since we are dealing with hundreds of thousands of examples and we are interested only in the scores themselves, this allows us to significantly reduce the computational cost, as the number of necessary tokens per judgment drops from ∼ 150 to just 10. Each evaluation is carried out independently.
